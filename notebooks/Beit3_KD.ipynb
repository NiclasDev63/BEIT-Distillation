{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8d9a98",
   "metadata": {},
   "source": [
    "# Knowledge Distillation on BEiT3\n",
    "This notebook implements the knowledge distillation between a BEiT3 VQA teacher and student on VizWiz. We used this in our project to train our KD student using the fine-tuned teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59854169-292f-4e43-9be1-9b342a533778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T10:34:55.400274Z",
     "start_time": "2024-07-19T10:34:55.383672Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current = os.getcwd()\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import datasets\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loading_beit3models import load_beit3_base, load_beit3_large\n",
    "from beit3_vizwiz_finetuning import initModel, getDataLoader, validate\n",
    "from transformers import BatchEncoding, XLMRobertaTokenizer\n",
    "from Beit3_vizwiz import VizWizDataset, get_img_names_and_questions\n",
    "import torch.nn as nn\n",
    "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a8312",
   "metadata": {},
   "source": [
    "Here we specify training settings such as hyperparameters (initialized from BEiT3 paper) and the student epoch to start the training with. Additionally we load the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d6fb8d-dcf3-439d-ba44-4d803c121ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T10:34:56.988247Z",
     "start_time": "2024-07-19T10:34:56.940377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n",
      "Loading student model..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './kd_student/vizwiz_checkpoint_kd_epoch6.tar'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m starting_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m6\u001B[39m\n\u001B[0;32m      6\u001B[0m ckp_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./kd_student/vizwiz_checkpoint_kd_epoch\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstarting_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.tar\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 7\u001B[0m student \u001B[38;5;241m=\u001B[39m \u001B[43minitModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mckp_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbase\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_compiled_model_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiling model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m student \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcompile(student)\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\beit3_vizwiz_finetuning.py:97\u001B[0m, in \u001B[0;36minitModel\u001B[1;34m(checkpoint_path, model_type, is_compiled_model_checkpoint)\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type must be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlarge\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 97\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mload_beit3_base_finetuned\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_compiled_model_checkpoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m model_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlarge\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     99\u001B[0m     model \u001B[38;5;241m=\u001B[39m load_beit3_large_finetuned(checkpoint_path, is_compiled_model_checkpoint)\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\loading_beit3models.py:104\u001B[0m, in \u001B[0;36mload_beit3_base_finetuned\u001B[1;34m(checkpoint_path, is_compiled_model_checkpoint)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_beit3_base_finetuned\u001B[39m(checkpoint_path: \u001B[38;5;28mstr\u001B[39m, is_compiled_model_checkpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    103\u001B[0m     device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 104\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    105\u001B[0m     model \u001B[38;5;241m=\u001B[39m beit3_base_patch16_480_vqav2()\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_compiled_model_checkpoint:\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\torch\\serialization.py:997\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    995\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 997\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    998\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    999\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\torch\\serialization.py:444\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\torch\\serialization.py:425\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 425\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './kd_student/vizwiz_checkpoint_kd_epoch6.tar'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "print(f'Loading student model..')\n",
    "#student = load_beit3_base()\n",
    "starting_epoch = 6\n",
    "if starting_epoch <= 0:\n",
    "    ckp_path = f'./models/base/beit3_base_indomain_patch16_480_vqa.pth'\n",
    "    is_trained = False\n",
    "else:\n",
    "    ckp_path = f'./kd_student/vizwiz_checkpoint_kd_epoch{starting_epoch}.tar'\n",
    "    is_trained = True\n",
    "student = initModel(ckp_path, 'base', is_compiled_model_checkpoint=True, is_pretrained=is_trained)\n",
    "print(\"compiling model...\")\n",
    "student = torch.compile(student)\n",
    "print(\"finished compiling\")\n",
    "# Using TensorFloat32 Cores for better performance\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# checkpoint = torch.load('beit-distillation/base/vizwiz_checkpoint_base_epoch6.tar')\n",
    "# student.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# print(f'Loading teacher model..')\n",
    "# teacher = initModel(f'/teamspace/studios/internal-yellow-dr2gf-zycm/beit-distillation/models/large/vizwiz_checkpoint_epoch15_large.tar', 'large', is_compiled_model_checkpoint=True)\n",
    "#parameters from beit3 vqva2 finetuning\n",
    "print(f'Prepping..')\n",
    "lr = 2e-5\n",
    "opt_betas = (0.9, 0.98)\n",
    "weight_decay = 0.01\n",
    "tokenizer = XLMRobertaTokenizer(\"../models/beit3.spm\")\n",
    "batch_size = 12\n",
    "\n",
    "print(f\"Loading dataset...\")\n",
    "vizwiz_path = \"/teamspace/studios/internal-yellow-dr2gf/VizWiz\"\n",
    "train_loader = getDataLoader(tokenizer=tokenizer, batch_size=batch_size, data_dir=vizwiz_path, split='train')\n",
    "val_loader = getDataLoader(tokenizer=tokenizer, batch_size=batch_size, data_dir=vizwiz_path, split='val')\n",
    "print(f\"Finished loading dataset.\")\n",
    "\n",
    "# freeze all parameters except for the head\n",
    "# for name, value in student.named_parameters():\n",
    "#    if not \"head\" in name:\n",
    "#        value.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e58a5",
   "metadata": {},
   "source": [
    "Here we define the KD training and evaluation loop, which in our case trains the entire student network on a linear combination between normal training loss on the labels and the loss between student and teacher logits. After each epoch the resulting checkpoint including training and validation loss is saved to the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4908baa2-5e2a-41ec-9927-ec33146d242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n",
    "def train_knowledge_distillation(student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, loss_weight, device, starting_epoch=0, checkpoint_path=None):\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    optim = torch.optim.AdamW(params = student.parameters(), lr = learning_rate, betas = opt_betas, weight_decay=weight_decay)\n",
    "    if checkpoint_path is not None:\n",
    "        ckp=torch.load(checkpoint_path)\n",
    "        optim.load_state_dict(ckp[\"optimizer_state_dict\"])\n",
    "    # since logits should not change between runs we simply read them out of the previously calculated file\n",
    "    all_teacher_logits = load_teacher_logits()\n",
    "    \n",
    "    \n",
    "    folder = f\"./kd_student\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    for epoch in range(starting_epoch+1, starting_epoch+epochs):\n",
    "        running_loss = 0.0\n",
    "        student.train() # Student to train mode\n",
    "        for data in tqdm(train_loader):\n",
    "            img = data[\"image\"].to(device)\n",
    "            q_tokens = data[\"language_tokens\"].to(device)\n",
    "            labels = data[\"labels\"].to(device)\n",
    "            optim.zero_grad()\n",
    "        \n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            # Since the teachers outputs should not change between epochs, we simply use the pre-calculated logits rather than recalculate them in every epoch.\n",
    "            # with torch.no_grad():\n",
    "            #    teacher_logits = teacher.forward(image=img, question=q_tokens)\n",
    "            #    teacher_logits = teacher_logits.float()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                teacher_logits = torch.stack([all_teacher_logits[id] for id in data[\"image_id\"]])\n",
    "                \n",
    "                # Forward pass with the student model\n",
    "                student_logits = student.forward(image=img, question=q_tokens)\n",
    "                student_logits = student_logits.float()\n",
    "                \n",
    "                #Soften the student logits by applying softmax first and log() second\n",
    "                soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "                soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "    \n",
    "                # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "                soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "    \n",
    "                # Calculate the true label loss\n",
    "                label_loss = criterion(student_logits, labels)\n",
    "    \n",
    "                # Weighted sum of the two losses\n",
    "                loss = soft_target_loss_weight * soft_targets_loss + loss_weight * label_loss\n",
    "    \n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            running_loss += loss.item()\n",
    "        student.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = validate(\n",
    "                tokenizer=tokenizer,\n",
    "                criterion=criterion,\n",
    "                model=student,\n",
    "                val_loader=val_loader,\n",
    "                device=device\n",
    "            )\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': running_loss / len(train_loader),\n",
    "            'val_loss': val_loss\n",
    "            }, os.path.join(folder, f'vizwiz_checkpoint_kd_epoch{epoch}.tar'))\n",
    "        print(f\"Epoch {epoch}/{epochs +starting_epoch}, Loss: {running_loss / len(train_loader)}, Val. Loss: {val_loss}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60bb2c",
   "metadata": {},
   "source": [
    "These functions are used to acquire the teacher logits on VizWiz for the KD training by inferencing with the teacher model. Once you have written the teacher logits you can simply load them, since they won't change as long as the teacher doesn't change, in which case you would need to write them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c34ff9-9e66-475d-88bb-451812adc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_teacher_logits(teacher, train_loader, device, filename=f'teacher_logits.tar'):\n",
    "    teacher.eval()\n",
    "    logits = {}\n",
    "    for data in tqdm(train_loader):\n",
    "        img = data[\"image\"].to(device)\n",
    "        q_tokens = data[\"language_tokens\"].to(device)\n",
    "        labels = data[\"labels\"].to(device)\n",
    "        with torch.no_grad():\n",
    "                teacher_logits = teacher.forward(image=img, question=q_tokens)\n",
    "                teacher_logits = teacher_logits.float()\n",
    "        for id, logs in zip(data[\"image_id\"], teacher_logits):\n",
    "            logits[id] = logs\n",
    "    torch.save(logits, filename)\n",
    "\n",
    "def load_teacher_logits(path='./teacher_logits.tar'):\n",
    "    return torch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de83895",
   "metadata": {},
   "source": [
    "Starts the training process. Additionally some KD specific training parameters including temperature, the weights for the combined loss and the number of epochs to train on are set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfabe4c-7465-48c6-8078-5a5adfa9b88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1711/1711 [26:03<00:00,  1.09it/s]\n",
      "100%|██████████| 360/360 [03:53<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0189\n",
      "Epoch 7/11, Loss: 0.10276553794210379, Val. Loss: 0.01893749316740367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1352/1711 [20:41<05:23,  1.11it/s]"
     ]
    }
   ],
   "source": [
    "T = 2\n",
    "soft_target_loss_weight = 0.25\n",
    "original_loss_weight = 1 - soft_target_loss_weight\n",
    "epochs = 5\n",
    "train_knowledge_distillation(student, train_loader, epochs, lr, T, soft_target_loss_weight, original_loss_weight, device, starting_epoch=starting_epoch, checkpoint_path=ckp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a3402",
   "metadata": {},
   "source": [
    "Gives an overview over all trained epochs and their respective losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e483dde-2527-4274-8ebd-3316885e87d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: vizwiz_checkpoint_kd_epoch1.tar, Epoch: 1, loss: 0.5062050504216947, val_loss: 0.02807389839629953\n",
      "File: vizwiz_checkpoint_kd_epoch10.tar, Epoch: 10, loss: 0.07444856881503123, val_loss: 0.01825543609625634\n",
      "File: vizwiz_checkpoint_kd_epoch2.tar, Epoch: 2, loss: 0.297218958086817, val_loss: 0.023673854132842582\n",
      "File: vizwiz_checkpoint_kd_epoch3.tar, Epoch: 3, loss: 0.21412093140520178, val_loss: 0.021490420229060368\n",
      "File: vizwiz_checkpoint_kd_epoch4.tar, Epoch: 4, loss: 0.16911141843056554, val_loss: 0.020520159855368546\n",
      "File: vizwiz_checkpoint_kd_epoch5.tar, Epoch: 5, loss: 0.1395778000389603, val_loss: 0.019529018430168636\n",
      "File: vizwiz_checkpoint_kd_epoch6.tar, Epoch: 6, loss: 0.11840906368229975, val_loss: 0.019089714166410785\n",
      "File: vizwiz_checkpoint_kd_epoch7.tar, Epoch: 7, loss: 0.10276553794210379, val_loss: 0.01893749316740367\n",
      "File: vizwiz_checkpoint_kd_epoch8.tar, Epoch: 8, loss: 0.0915182161203154, val_loss: 0.018791058071656153\n",
      "File: vizwiz_checkpoint_kd_epoch9.tar, Epoch: 9, loss: 0.08206946248420494, val_loss: 0.018600186800015055\n"
     ]
    }
   ],
   "source": [
    "student_dir = './kd_student'\n",
    "for file in os.listdir(student_dir):\n",
    "    ckp = torch.load(os.path.join(student_dir, file),map_location=torch.device('cpu'))\n",
    "    print(f\"File: {file}, Epoch: {ckp['epoch']}, loss: {ckp['loss']}, val_loss: {ckp['val_loss']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
