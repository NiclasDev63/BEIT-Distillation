{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759c0e9f",
   "metadata": {},
   "source": [
    "# Finetuning BEiT3\n",
    "This notebook implements the fine-tuning of the BEiT3 VQA model on VizWiz. We used this in our project to fine-tune the teacher and the base model used for comparison to the KD student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T10:35:25.206897Z",
     "start_time": "2024-07-19T10:35:25.195775Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from beit3_vizwiz_finetuning import initModel, freeze_until, initOptimizerLoss, getDataLoader, validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2003ed7",
   "metadata": {},
   "source": [
    "Here we specify training settings such as hyperparameters (initialized from BEiT3 paper) and the amount of epochs we want to fine-tune while freezing all encoder layers up to 'last_unfrozen_layer'. You also need to specify whether you want to train the large or the base variant of VQA-BEiT3. Additionally you should specify the epoch number of the checkpoint you want to start the fine-tuning on or 0 if you want to use the model provided by BEiT without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3dd63cd4445bed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T10:35:27.077623Z",
     "start_time": "2024-07-19T10:35:27.049294Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"./models/beit3.spm\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m embedding_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./models/beit3.spm\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mXLMRobertaTokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m last_unfrozen_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m20\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      7\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta.py:123\u001B[0m, in \u001B[0;36mXLMRobertaTokenizer.__init__\u001B[1;34m(self, vocab_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, sp_model_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model_kwargs \u001B[38;5;241m=\u001B[39m {} \u001B[38;5;28;01mif\u001B[39;00m sp_model_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sp_model_kwargs\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model \u001B[38;5;241m=\u001B[39m spm\u001B[38;5;241m.\u001B[39mSentencePieceProcessor(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model_kwargs)\n\u001B[1;32m--> 123\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msp_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoad\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;241m=\u001B[39m vocab_file\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# Original fairseq vocab and spm vocab must be \"aligned\":\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;66;03m# Vocab    |    0    |    1    |   2    |    3    |  4  |  5  |  6  |   7   |   8   |  9\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# -------- | ------- | ------- | ------ | ------- | --- | --- | --- | ----- | ----- | ----\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    131\u001B[0m \n\u001B[0;32m    132\u001B[0m \u001B[38;5;66;03m# Mimic fairseq token-to-id alignment for the first 4 token\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\sentencepiece\\__init__.py:961\u001B[0m, in \u001B[0;36mSentencePieceProcessor.Load\u001B[1;34m(self, model_file, model_proto)\u001B[0m\n\u001B[0;32m    959\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_proto:\n\u001B[0;32m    960\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLoadFromSerializedProto(model_proto)\n\u001B[1;32m--> 961\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Projects\\MAI\\beit-distillation\\.venvnew\\lib\\site-packages\\sentencepiece\\__init__.py:316\u001B[0m, in \u001B[0;36mSentencePieceProcessor.LoadFromFile\u001B[1;34m(self, arg)\u001B[0m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadFromFile\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg):\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_sentencepiece\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSentencePieceProcessor_LoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOSError\u001B[0m: Not found: \"./models/beit3.spm\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = \"./models/beit3.spm\"\n",
    "tokenizer = XLMRobertaTokenizer(embedding_model)\n",
    "\n",
    "last_unfrozen_layer = '20'\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "model_type = \"large\"\n",
    "epoch_of_checkpoint = 12\n",
    "if(epoch_of_checkpoint <=0):\n",
    "    checkpoint_path = f\"./models/{model_type}/beit3_{model_type}_indomain_patch16_480_vqa.pth\"\n",
    "    is_pretrained = False\n",
    "else:   \n",
    "    checkpoint_path = f\"./models/{model_type}/vizwiz_checkpoint_epoch{epoch_of_checkpoint}_{model_type}.tar\"\n",
    "    is_pretrained = True\n",
    "vizwiz_path = \"./VizWiz/\"\n",
    "save_checkpoint_folder = f\"./models/{model_type}\"\n",
    "\n",
    "#parameters from beit3 vqav2 finetuning\n",
    "lr = 2e-5\n",
    "opt_betas = (0.9, 0.98)\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8163706",
   "metadata": {},
   "source": [
    "Loading and initializing the model as well as the optimizer and the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c7fff345fa6fad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T10:35:27.772781Z",
     "start_time": "2024-07-19T10:35:27.746973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading model..\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m initModel(\u001B[43mcheckpoint_path\u001B[49m, model_type, is_compiled_model_checkpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# train layers 20, 21, 22, 23 and head\u001B[39;00m\n\u001B[0;32m      4\u001B[0m freeze_until(model, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeit3.encoder.layers.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlast_unfrozen_layer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'checkpoint_path' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model..\")\n",
    "model = initModel(checkpoint_path, model_type, is_compiled_model_checkpoint=True, is_pretrained=is_pretrained)\n",
    "# only unfreezes the specified encoder layers as well as the decoder head\n",
    "freeze_until(model, f\"beit3.encoder.layers.{last_unfrozen_layer}\")\n",
    "print(\"Compiling model...\")\n",
    "model = torch.compile(model)\n",
    "print(\"Finished compiling\")\n",
    "# Using TensorFloat32 Cores for better performance\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# optim, criterion = initOptimizerLoss(model, checkpoint_path)\n",
    "optim, criterion = initOptimizerLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaaec1fd4772e281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T10:13:11.582707Z",
     "start_time": "2024-06-24T10:13:11.577658Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beit3.encoder.layers.22.self_attn.k_proj.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.k_proj.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.k_proj.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.k_proj.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.v_proj.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.v_proj.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.v_proj.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.v_proj.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.q_proj.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.q_proj.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.q_proj.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.q_proj.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.out_proj.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.out_proj.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.out_proj.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.out_proj.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.inner_attn_ln.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.inner_attn_ln.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.inner_attn_ln.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn.inner_attn_ln.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn_layer_norm.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn_layer_norm.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn_layer_norm.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.self_attn_layer_norm.B.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.fc1.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.fc1.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.fc2.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.fc2.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.ffn_layernorm.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.A.ffn_layernorm.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.fc1.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.fc1.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.fc2.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.fc2.bias\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.ffn_layernorm.weight\n",
      "True\n",
      "beit3.encoder.layers.22.ffn.B.ffn_layernorm.bias\n",
      "True\n",
      "beit3.encoder.layers.22.final_layer_norm.A.weight\n",
      "True\n",
      "beit3.encoder.layers.22.final_layer_norm.A.bias\n",
      "True\n",
      "beit3.encoder.layers.22.final_layer_norm.B.weight\n",
      "True\n",
      "beit3.encoder.layers.22.final_layer_norm.B.bias\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_loader = getDataLoader(tokenizer=tokenizer, batch_size=batch_size, data_dir=vizwiz_path, split='train')\n",
    "losses = []\n",
    "num_correct = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75193b",
   "metadata": {},
   "source": [
    "Training and validation loop over the specified number of epochs. After each epoch a new checkpoint including the models training and evaluation loss for that epoch is saved to the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf2c3bdda8c66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T10:13:44.394255Z",
     "start_time": "2024-06-24T10:13:11.609191Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 514/642 [26:00<06:34,  3.08s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Starting training\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for data in tqdm(train_loader):\n",
    "\n",
    "        img = data[\"image\"].to(device)\n",
    "        q_tokens = data[\"language_tokens\"].to(device)\n",
    "        labels = data[\"labels\"].to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(image=img, question=q_tokens)\n",
    "            logits = logits.float()\n",
    "            loss = criterion(input=logits, target=labels)\n",
    "            epoch_loss += loss.item() * img.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = validate(\n",
    "            tokenizer=tokenizer,\n",
    "            criterion=criterion,\n",
    "            batch_size=batch_size//2,\n",
    "            model=model,\n",
    "            data_dir=vizwiz_path\n",
    "        )\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optim.state_dict(),\n",
    "            \"loss\": epoch_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        },\n",
    "        os.path.join(\n",
    "            save_checkpoint_folder, f\"vizwiz_checkpoint_epoch{epoch + 1 + epoch_of_checkpoint}_{model_type}.tar\"\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Epoch {epoch} loss: {epoch_loss}\")\n",
    "    epoch_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
